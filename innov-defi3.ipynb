{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a8f62-d907-41aa-8287-e46f9bedae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from io import open\n",
    "\n",
    "train_data = pd.read_csv(\"data/Ashley-Madison.txt\", header=None, names=['input'])\n",
    "train_data['input'] = train_data['input'].astype(str)\n",
    "\n",
    "print(\"La taille du dataframe est : \", len(train_data))\n",
    "\n",
    "def count_words_by_length(filename):\n",
    "    word_length_counts = {}\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        words = file.read().split()\n",
    "\n",
    "    for word in words:\n",
    "        length = len(word)\n",
    "        if length in word_length_counts:\n",
    "            word_length_counts[length] += 1\n",
    "        else:\n",
    "            word_length_counts[length] = 1\n",
    "\n",
    "    return word_length_counts\n",
    "\n",
    "resultat = count_words_by_length(\"data/Ashley-Madison.txt\")\n",
    "\n",
    "\n",
    "sorted_counts = sorted(resultat.items())\n",
    "\n",
    "\n",
    "print(\"Longueur  | Nombre de mots\")\n",
    "print(\"-------------------------\")\n",
    "for length, count in sorted_counts:\n",
    "    print(f\"{length:8} | {count}\")\n",
    "\n",
    "train_data['length'] = train_data['input'].str.len()\n",
    "\n",
    "\n",
    "mean_df = train_data[\"length\"].mean()\n",
    "print(\"La moyenne des MDP est : \", mean_df)\n",
    "\n",
    "\n",
    "index_max_length = train_data['length'].idxmax()\n",
    "\n",
    "\n",
    "input_max_length = train_data.loc[index_max_length, 'input']\n",
    "\n",
    "print(\"L'input le plus long est:\", input_max_length, \"et contient : \", train_data.loc[index_max_length, 'length'], \"caractere\")\n",
    "\n",
    "\n",
    "word_count_by_length = train_data['length'].value_counts().sort_index()\n",
    "\n",
    "plt.xlim(0, 110)\n",
    "plt.ylim(0, 100000)\n",
    "\n",
    "\n",
    "plt.bar(word_count_by_length.index, word_count_by_length.values)\n",
    "\n",
    "\n",
    "plt.xlabel('Taille du mot')\n",
    "plt.ylabel('Nombre de mots')\n",
    "plt.title('Nombre de mots par taille')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "t with open(train_data, 'r', encoding='utf-8') as file:\n",
    "        corpus = file.read()\n",
    "\n",
    "   \n",
    "    mots = corpus.split()\n",
    "\n",
    " \n",
    "    lettres_seulement = 0\n",
    "    chiffres_seulement = 0\n",
    "    caracteres_speciaux = 0\n",
    "    combinaisons = 0\n",
    "    caracteres_speciaux_et_chiffres = 0\n",
    "    caracteres_speciaux_et_lettres = 0\n",
    "    caracteres_speciaux_et_chiffres_et_lettres = 0\n",
    "\n",
    "    # Liste pour stocker les catégories des mots\n",
    "    categories = []\n",
    "\n",
    "    # Expression régulière pour les lettres uniquement\n",
    "    lettre_pattern = re.compile(\"^[a-zA-Z]+$\")\n",
    "\n",
    "    # Expression régulière pour les chiffres uniquement\n",
    "    chiffre_pattern = re.compile(\"^[0-9]+$\")\n",
    "\n",
    "    for mot in mots:\n",
    "        cat_added = False\n",
    "        if lettre_pattern.match(mot):\n",
    "            categories.append(\"Lettres Seulement\")\n",
    "            lettres_seulement += 1\n",
    "            cat_added = True\n",
    "        elif chiffre_pattern.match(mot):\n",
    "            categories.append(\"Chiffres Seulement\")\n",
    "            chiffres_seulement += 1\n",
    "            cat_added = True\n",
    "        elif mot.isalpha():\n",
    "            categories.append(\"Combinaison de Lettres\")\n",
    "            combinaisons += 1\n",
    "            cat_added = True\n",
    "        elif mot.isalnum():\n",
    "            categories.append(\"Combinaison de Lettres et Chiffres\")\n",
    "            combinaisons += 1\n",
    "            cat_added = True\n",
    "        elif not mot.isalnum():\n",
    "            categories.append(\"Caracteres Speciaux\")\n",
    "            caracteres_speciaux += 1\n",
    "            cat_added = True\n",
    "        elif any(char.isdigit() for char in mot) and any(char.isalpha() or not char.isalnum() for char in mot):\n",
    "            categories.append(\"Caracteres Speciaux et Chiffres\")\n",
    "            caracteres_speciaux_et_chiffres += 1\n",
    "            cat_added = True\n",
    "        elif any(char.isdigit() for char in mot) and any(char.isalpha() for char in mot):\n",
    "            categories.append(\"Caracteres Speciaux, Chiffres et Lettres\")\n",
    "            caracteres_speciaux_et_chiffres_et_lettres += 1\n",
    "            cat_added = True\n",
    "        elif any(char.isalpha() for char in mot) and not mot.isalnum():\n",
    "            categories.append(\"Caracteres Speciaux et Lettres\")\n",
    "            caracteres_speciaux_et_lettres += 1\n",
    "            cat_added = True\n",
    "\n",
    "        # Si aucune catégorie n'a été ajoutée, ajouter une catégorie par défaut\n",
    "        if not cat_added:\n",
    "            categories.append(\"Autre\")\n",
    "\n",
    "    # Afficher les résultats\n",
    "    print(\"Nombre de mots contenant uniquement des lettres : \", lettres_seulement)\n",
    "    print(\"Nombre de mots contenant uniquement des chiffres : \", chiffres_seulement)\n",
    "    print(\"Nombre de mots contenant des caractères spéciaux : \", caracteres_speciaux)\n",
    "    print(\"Nombre de mots contenant des combinaisons de lettres et chiffres : \", combinaisons)\n",
    "    print(\"Nombre de mots contenant des caractères spéciaux et chiffres : \", caracteres_speciaux_et_chiffres)\n",
    "    print(\"Nombre de mots contenant des caractères spéciaux, chiffres et lettres : \", caracteres_speciaux_et_chiffres_et_lettres)\n",
    "    print(\"Nombre de mots contenant des caractères spéciaux et lettres : \", caracteres_speciaux_et_lettres)\n",
    "\n",
    "duplicated_count = train_data['input'].duplicated().sum()\n",
    "print(\"Nombre de mots redondants :\", duplicated_count)\n",
    "\n",
    "\n",
    "train_data = train_data[train_data[\"input\"].str.len() >= 4]\n",
    "\n",
    "train_data = train_data[train_data[\"input\"].str.len() <= 20]\n",
    "print(\"La taille du dataframe apres modification est : \", len(train_data))\n",
    "\n",
    "\n",
    "train_data[\"target\"] = train_data[\"input\"] + \"\\n\"\n",
    "\n",
    "\n",
    "train_data[\"input\"] = \"\\t\" + train_data[\"input\"]\n",
    "\n",
    "\n",
    "contains_space = train_data['input'].str.contains(' ')\n",
    "\n",
    "\n",
    "rows_with_space = train_data[contains_space]\n",
    "print(rows_with_space)\n",
    "\n",
    "chars = sorted(list(set(\"\\n\".join(train_data[\"input\"]))))\n",
    "vocab_size = len(chars)\n",
    "print(\"Total chars:\", vocab_size)\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for line in train_data[\"input\"]:\n",
    "    tmp = [char for char in line]\n",
    "    inputs.append(tmp)\n",
    "\n",
    "for line in train_data[\"target\"]:\n",
    "    tmp = [char for char in line]\n",
    "    outputs.append(tmp)\n",
    "\n",
    "X_train = tf.ragged.constant(inputs)\n",
    "X_train = tf.one_hot(X_train, depth=vocab_size)\n",
    "\n",
    "y_train = tf.ragged.constant(outputs)\n",
    "y_train = tf.one_hot(y_train, depth=vocab_size)\n",
    "\n",
    "def onehot2indices(hotmax):\n",
    "    return tf.math.argmax(hotmax, axis=-1).numpy().flatten()\n",
    "\n",
    "def onehot2chars(hotmax):\n",
    "    return [indices_char[i] for i in onehot2indices(hotmax)]\n",
    "\n",
    "def word2indices(word):\n",
    "    return [char_indices[c] for c in word]\n",
    "\n",
    "def indices2word(indices):\n",
    "    return ''.join([indices_char[i] for i in indices])\n",
    "\n",
    "# Fonction pour créer la matrice one-hot d'un mot\n",
    "def mot_to_onehot(mot, char_indices):\n",
    "    vocab_size = len(char_indices)\n",
    "    onehot_matrix = [keras.utils.to_categorical(char_indices[char], num_classes=vocab_size) for char in mot]\n",
    "    return onehot_matrix\n",
    "\n",
    "mot = \"salut\"\n",
    "matrice_onehot = mot_to_onehot(mot, char_indices)\n",
    "print(matrice_onehot)\n",
    "\n",
    "onehot2chars(matrice_onehot)\n",
    "\n",
    "def get_rnn():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=(None, vocab_size)))\n",
    "    model.add(layers.SimpleRNN(128, return_sequences=True))  \n",
    "    model.add(layers.Dense(vocab_size, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "rnn_model = get_lstm()\n",
    "\n",
    "def get_lstm():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=(None, vocab_size)))\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "    model.add(layers.Dense(vocab_size, activation=\"softmax\"))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "lstm_model = get_lstm()\n",
    "\n",
    "def get_bidirectional_lstm():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=(None, vocab_size)))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True)))\n",
    "    model.add(layers.Dense(vocab_size, activation=\"softmax\"))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "bidirectional_lstm = get_bidirectional_lstm()\n",
    "\n",
    "def get_stacked_lstm():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=(None, vocab_size)))\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "    model.add(layers.Dense(vocab_size, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "stacked_lstm = get_stacked_lstm()\n",
    "\n",
    "epochs = 30  # A modifier\n",
    "lstm_h128_b32_model = lstm_model.fit(X_train, y_train, batch_size=32, epochs=epochs)\n",
    "\n",
    "model_name = \"lstm_h128_b32\"\n",
    "\n",
    "def save_model(model, file_path):\n",
    "    model.save('models/lstm_h128_b32/lstm_h128_b32_e15.h5')\n",
    "    print(f\"Model saved to {file_path}\")\n",
    "\n",
    "file_path = 'models/lstm_h128_b32'\n",
    "save_model(lstm_model, file_path)\n",
    "\n",
    "def get_model(file=None, epoch=None):\n",
    "    if file is not None:\n",
    "        return keras.models.load_model(\"models/{}/{}_e{}.h5\".format(file, file, epoch))\n",
    "    return get_RNN()\n",
    "    \n",
    "   #return bidirectional_lstm\n",
    "   #return lstm_model\n",
    "\n",
    "model.save(\"models/{}/{}_e{}.h5\".format(model_name, model_name, epoch))\n",
    "\n",
    "def generate_word(model, max_length=20):\n",
    "    out = [0]  # Start with \\t token\n",
    "    input = tf.one_hot(out, depth=vocab_size)  # One hot encode\n",
    "    input = tf.reshape(input, (1, input.shape[0], input.shape[1]))  # Add batch dimension\n",
    "    for i in range(max_length):\n",
    "        prediction = model.predict(input, verbose=0)  # Predict next letter\n",
    "        prediction = prediction[:, -1:, :][0]  # Last letter softmax probabilities\n",
    "        id = tf.squeeze(tf.random.categorical(tf.math.log(prediction), 1)).numpy()  # Sample from softmax\n",
    "        if indices_char[id] == \"\\n\":  # Stop if \\n token is predicted\n",
    "            break\n",
    "        out.append(id)  # Append predicted letter\n",
    "        input = tf.one_hot(out, depth=vocab_size)  # Update input\n",
    "        input = tf.reshape(input, (1, input.shape[0], input.shape[1]))\n",
    "    out = out[1:]  # Remove \\t token\n",
    "    del input, prediction, id\n",
    "    return indices2word(out)\n",
    "\n",
    "generate_word(stacked_lstm, 12)\n",
    "\n",
    "def generate_n_words(model, n, max_length=20):\n",
    "    words = []\n",
    "    for i in range(n):\n",
    "        print(\"Generating word {}/{}\".format(i, n), end=\"\\r\")\n",
    "        words.append(generate_word(model, max_length))\n",
    "    return words\n",
    "\n",
    "def generate_and_save_words(model, num_words, max_length=20, output_file=\"generated_words.txt\"):\n",
    "    generated_words = generate_n_words(model, num_words, max_length)\n",
    "    with open(output_file, \"w\") as file:\n",
    "        for word in generated_words:\n",
    "            file.write(word + \"\\n\")\n",
    "    print(f\"{num_words} ont été générés et sauvegardés dans {output_file}\")\n",
    "\n",
    "generate_and_save_words(lstm_model, num_words=10000, max_length=20, output_file=\"rendu/10K.txt\")\n",
    "\n",
    "generate_and_save_words(lstm_model, num_words=100000, max_length=20, output_file=\"rendu/100K.txt\")\n",
    "\n",
    "generate_and_save_words(lstm_model, num_words=1000000, max_length=20, output_file=\"rendu/1M.txt\")\n",
    "\n",
    "def compare_passwords(file1_path, file2_path):\n",
    "    with open(file1_path, \"r\", encoding=\"latin-1\") as file1:\n",
    "        passwords1 = set(file1.read().splitlines())\n",
    "\n",
    "    with open(file2_path, \"r\", encoding=\"latin-1\") as file2:\n",
    "        passwords2 = set(file2.read().splitlines())\n",
    "\n",
    "    common_passwords = passwords1.intersection(passwords2)\n",
    "    return len(common_passwords)\n",
    "\n",
    "file1_path = \"rendu/100K.txt\"\n",
    "file2_path = \"data/Ashley-Madison.txt\"\n",
    "result = compare_passwords(file1_path, file2_path)\n",
    "\n",
    "print(\"Nombre de mots de passe identiques :\", result)\n",
    "\n",
    "def calculate_accuracy_and_matches(original_dataset_path, generated_file_path):\n",
    "    # Load the original dataset\n",
    "    with open(original_dataset_path, 'r', encoding='latin-1') as file:\n",
    "        original_passwords = set(file.read().splitlines())\n",
    "\n",
    "    # Load generated passwords\n",
    "    with open(generated_file_path, 'r', encoding='latin-1') as file:\n",
    "        generated_passwords = set(file.read().splitlines())\n",
    "\n",
    "    # Find matches using set intersection\n",
    "    matching_passwords = original_passwords.intersection(generated_passwords)\n",
    "\n",
    "    # Calculate accuracy based on full line matches\n",
    "    accuracy = (len(matching_passwords) / len(generated_passwords)) * 100 if generated_passwords else 0\n",
    "    return accuracy, matching_passwords\n",
    "\n",
    "# Paths to the files\n",
    "original_dataset_path = \"data/rockyou.txt\"     #\"data/Ashley-Madison.txt\"\n",
    "generated_file_path = \"rendu/e30/100K.txt\"\n",
    "\n",
    "# Calculate accuracy and get matches\n",
    "accuracy, matching_passwords = calculate_accuracy_and_matches(original_dataset_path, generated_file_path)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(\"Matching Passwords Number:\", len(matching_passwords))\n",
    "print(\"Matching Passwords:\", matching_passwords)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
